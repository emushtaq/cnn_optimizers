# Optimizers - A comparison
My simple comparison of the Adam, Adadelta and SGD optimizers used in a CNN based MNIST classifier.

Visualizing Accuracy and Loss *:

1. SGD OPTIMIZER

![alt text](https://github.com/emushtaq/comparing_cnn_optimizers/blob/master/Results/accuracy_sgd.png)
![alt text](https://github.com/emushtaq/comparing_cnn_optimizers/blob/master/Results/loss_sgd.png)

2. ADAM OPTIMIZER

![alt text](https://github.com/emushtaq/comparing_cnn_optimizers/blob/master/Results/accuracy_adam.png)
![alt text](https://github.com/emushtaq/comparing_cnn_optimizers/blob/master/Results/loss_adam.png)

3. ADADELTA OPTIMIZER

![alt text](https://github.com/emushtaq/comparing_cnn_optimizers/blob/master/Results/accuracy_adadelta.png)
![alt text](https://github.com/emushtaq/comparing_cnn_optimizers/blob/master/Results/loss_adadelta.png)



\* Tuning the right hyperparams and altering the number of epochs and batch size could improve results vastly.
